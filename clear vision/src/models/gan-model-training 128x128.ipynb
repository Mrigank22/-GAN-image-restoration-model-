{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11943913,"sourceType":"datasetVersion","datasetId":7508568},{"sourceId":11944481,"sourceType":"datasetVersion","datasetId":7508956},{"sourceId":411325,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":335822,"modelId":356829}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport os\nimport yaml\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.utils import save_image\nfrom PIL import Image\nimport sys\nsys.path.append('/kaggle/input/gan_model_1/pytorch/default/1')\nfrom gan_model import UNetGenerator, PatchDiscriminator\nfrom torchvision import transforms\nfrom pathlib import Path","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T11:19:42.264733Z","iopub.execute_input":"2025-05-25T11:19:42.265232Z","iopub.status.idle":"2025-05-25T11:19:48.936255Z","shell.execute_reply.started":"2025-05-25T11:19:42.265203Z","shell.execute_reply":"2025-05-25T11:19:48.935314Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"class ImagePairDataset(Dataset):\n    def __init__(self, pair_file, corrupted_root, raw_root, image_size=(256, 256)):\n        if isinstance(image_size, int):\n            image_size = (image_size, image_size)\n        self.image_size = image_size\n        self.pairs = []\n\n        # Convert roots to Path objects\n        corrupted_root = Path(corrupted_root)\n        raw_root = Path(raw_root)\n\n        with open(pair_file, \"r\") as f:\n            for line in f:\n                raw_rel, corrupted_rel = line.strip().split(',')\n\n                # Remove any backslashes in relative paths from Windows systems\n                raw_rel = raw_rel.strip().replace(\"\\\\\", \"/\")\n                corrupted_rel = corrupted_rel.strip().replace(\"\\\\\", \"/\")\n\n                # Create full paths and convert to POSIX\n                raw_path = (raw_root / raw_rel).as_posix()\n                corrupted_path = (corrupted_root / corrupted_rel).as_posix()\n\n                self.pairs.append((raw_path, corrupted_path))\n\n        self.corrupted_root = corrupted_root\n        self.raw_root = raw_root\n\n        # Improved transform pipeline with normalization\n        self.transform = transforms.Compose([\n            transforms.Resize((256, 256)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n        ])\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        raw_rel, corrupted_rel = self.pairs[idx]\n\n        # Only keep the filename (strip full/relative paths)\n        raw_filename = os.path.basename(raw_rel)\n        corrupted_filename = os.path.basename(corrupted_rel)\n\n        raw_path = Path(self.raw_root) / raw_filename\n        corrupted_path = Path(self.corrupted_root) / corrupted_filename\n\n        try:\n            raw_img = Image.open(raw_path).convert(\"RGB\")\n            corrupted_img = Image.open(corrupted_path).convert(\"RGB\")\n        except Exception as e:\n            print(f\"Error loading images: {raw_path}, {corrupted_path}\")\n            raise e\n\n        # Apply transforms (resize + to tensor + normalize)\n        raw_img = self.transform(raw_img)\n        corrupted_img = self.transform(corrupted_img)\n\n        return corrupted_img, raw_img\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T11:19:48.937805Z","iopub.execute_input":"2025-05-25T11:19:48.938268Z","iopub.status.idle":"2025-05-25T11:19:48.948642Z","shell.execute_reply.started":"2025-05-25T11:19:48.938239Z","shell.execute_reply":"2025-05-25T11:19:48.947664Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"\ndef train():\n    with open(\"/kaggle/input/configsetting/config/gan_config.yaml\") as f:\n        config = yaml.safe_load(f)\n\n    # Ensure image_size from config is a tuple\n    image_size = config.get(\"image_size\", (256, 256))\n    if isinstance(image_size, int):\n        image_size = (image_size, image_size)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n    \n    G = UNetGenerator().to(device)\n    D = PatchDiscriminator().to(device)\n\n    # Initialize weights properly\n    def weights_init(m):\n        classname = m.__class__.__name__\n        if classname.find('Conv') != -1:\n            nn.init.normal_(m.weight.data, 0.0, 0.02)\n        elif classname.find('BatchNorm') != -1:\n            nn.init.normal_(m.weight.data, 1.0, 0.02)\n            nn.init.constant_(m.bias.data, 0)\n\n    G.apply(weights_init)\n    D.apply(weights_init)\n\n    # Use MSE loss for more stable training\n    criterion_GAN = torch.nn.MSELoss()  # Changed from BCELoss\n    criterion_L1 = torch.nn.L1Loss()\n\n    # Different learning rates for G and D\n    lr_g = config.get(\"learning_rate_g\", config.get(\"learning_rate\", 0.0002))\n    lr_d = config.get(\"learning_rate_d\", config.get(\"learning_rate\", 0.0002))\n    \n    optimizer_G = torch.optim.Adam(G.parameters(), lr=lr_g, betas=(config.get(\"beta1\", 0.5), 0.999))\n    optimizer_D = torch.optim.Adam(D.parameters(), lr=lr_d, betas=(config.get(\"beta1\", 0.5), 0.999))\n\n    # Add learning rate schedulers\n    scheduler_G = torch.optim.lr_scheduler.StepLR(optimizer_G, step_size=50, gamma=0.5)\n    scheduler_D = torch.optim.lr_scheduler.StepLR(optimizer_D, step_size=50, gamma=0.5)\n\n    train_dataset = ImagePairDataset(\n        config[\"train_pairs_path\"],\n        config[\"corrupted_image_root\"],\n        config[\"raw_image_root\"],\n        image_size=image_size\n    )\n    train_loader = DataLoader(\n        train_dataset, \n        batch_size=config[\"batch_size\"], \n        shuffle=True,\n        num_workers=2,  # Add parallel data loading\n        pin_memory=True if torch.cuda.is_available() else False\n    )\n\n    os.makedirs(config[\"checkpoint_dir\"], exist_ok=True)\n    os.makedirs(config[\"results_dir\"], exist_ok=True)\n\n    # Training loop with improvements\n    for epoch in range(config[\"epochs\"]):\n        G.train()\n        D.train()\n        \n        epoch_g_loss = 0\n        epoch_d_loss = 0\n        \n        for i, (x, y) in enumerate(train_loader):\n            x, y = x.to(device), y.to(device)\n            batch_size = x.size(0)\n\n            # Create simple labels for discriminator\n            real_labels = torch.ones(batch_size, 1, device=device)\n            fake_labels = torch.zeros(batch_size, 1, device=device)\n\n            # Update Discriminator more frequently than Generator\n            for _ in range(1):  # You can increase this to 2 or 3 if D is too weak\n                D.zero_grad()\n                \n                # Real pairs\n                pred_real = D(x, y)\n                loss_D_real = criterion_GAN(pred_real, real_labels)\n                \n                # Fake pairs\n                with torch.no_grad():\n                    fake_img = G(x)\n                pred_fake = D(x, fake_img)\n                loss_D_fake = criterion_GAN(pred_fake, fake_labels)\n                \n                loss_D = (loss_D_real + loss_D_fake) * 0.5\n                loss_D.backward()\n                optimizer_D.step()\n\n            # Update Generator\n            G.zero_grad()\n            fake_img = G(x)\n            pred_fake = D(x, fake_img)\n            \n            # GAN loss\n            loss_G_GAN = criterion_GAN(pred_fake, real_labels)\n            \n            # L1 loss\n            lambda_L1 = config.get(\"lambda_L1\", 100)  # Default value\n            loss_G_L1 = criterion_L1(fake_img, y) * lambda_L1\n            \n            # Perceptual loss (optional - requires additional implementation)\n            # loss_G_perceptual = perceptual_loss(fake_img, y) * lambda_perceptual\n            \n            loss_G = loss_G_GAN + loss_G_L1\n            loss_G.backward()\n            optimizer_G.step()\n\n            epoch_g_loss += loss_G.item()\n            epoch_d_loss += loss_D.item()\n\n            if i % 100 == 0:\n                print(f\"Epoch [{epoch}/{config['epochs']}], Step [{i}], \"\n                      f\"D Loss: {loss_D.item():.4f}, G Loss: {loss_G.item():.4f}, \"\n                      f\"G_GAN: {loss_G_GAN.item():.4f}, G_L1: {loss_G_L1.item():.4f}\")\n                \n                # Save images with proper denormalization\n                with torch.no_grad():\n                    fake_img_denorm = fake_img * 0.5 + 0.5  # Denormalize from [-1,1] to [0,1]\n                    real_img_denorm = y * 0.5 + 0.5\n                    input_img_denorm = x * 0.5 + 0.5\n                    \n                    # Save comparison\n                    comparison = torch.cat([input_img_denorm[:4], fake_img_denorm[:4], real_img_denorm[:4]], dim=0)\n                    save_image(comparison, f\"{config['results_dir']}/comparison_{epoch}_{i}.png\", nrow=4)\n\n        # Update learning rates\n        scheduler_G.step()\n        scheduler_D.step()\n        \n        avg_g_loss = epoch_g_loss / len(train_loader)\n        avg_d_loss = epoch_d_loss / len(train_loader)\n        print(f\"Epoch [{epoch}] completed - Avg G Loss: {avg_g_loss:.4f}, Avg D Loss: {avg_d_loss:.4f}\")\n\n        # Save checkpoints\n        if epoch % 10 == 0 or epoch == config[\"epochs\"] - 1:\n            # Save comprehensive checkpoint\n            torch.save({\n                'generator_state_dict': G.state_dict(),\n                'discriminator_state_dict': D.state_dict(),\n                'optimizer_G_state_dict': optimizer_G.state_dict(),\n                'optimizer_D_state_dict': optimizer_D.state_dict(),\n                'epoch': epoch,\n            }, f\"{config['checkpoint_dir']}/checkpoint_epoch_{epoch}.pth\")\n            \n            # Also save generator only (like your original code)\n            torch.save(G.state_dict(), f\"{config['checkpoint_dir']}/G_epoch{epoch}.pth\")\n            \n            # Save discriminator separately too\n            torch.save(D.state_dict(), f\"{config['checkpoint_dir']}/D_epoch{epoch}.pth\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T11:19:48.949782Z","iopub.execute_input":"2025-05-25T11:19:48.950146Z","iopub.status.idle":"2025-05-25T11:19:48.995691Z","shell.execute_reply.started":"2025-05-25T11:19:48.950117Z","shell.execute_reply":"2025-05-25T11:19:48.994852Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T11:19:48.997333Z","iopub.execute_input":"2025-05-25T11:19:48.997697Z","iopub.status.idle":"2025-05-25T14:34:35.041715Z","shell.execute_reply.started":"2025-05-25T11:19:48.997671Z","shell.execute_reply":"2025-05-25T14:34:35.040199Z"}},"outputs":[{"name":"stdout","text":"Using device: cpu\nEpoch [0/100], Step [0], D Loss: 0.3651, G Loss: 67.3083, G_GAN: 3.5252, G_L1: 63.7831\nEpoch [0] completed - Avg G Loss: 40.9190, Avg D Loss: 0.6743\nEpoch [1/100], Step [0], D Loss: 0.2340, G Loss: 25.5721, G_GAN: 0.4450, G_L1: 25.1271\nEpoch [1] completed - Avg G Loss: 26.8116, Avg D Loss: 0.2353\nEpoch [2/100], Step [0], D Loss: 0.2935, G Loss: 13.1667, G_GAN: 0.1037, G_L1: 13.0630\nEpoch [2] completed - Avg G Loss: 23.1379, Avg D Loss: 0.2295\nEpoch [3/100], Step [0], D Loss: 0.5829, G Loss: 25.6045, G_GAN: 0.3827, G_L1: 25.2219\nEpoch [3] completed - Avg G Loss: 21.8384, Avg D Loss: 0.2248\nEpoch [4/100], Step [0], D Loss: 0.1417, G Loss: 20.4687, G_GAN: 0.6069, G_L1: 19.8617\nEpoch [5/100], Step [0], D Loss: 0.1956, G Loss: 24.5694, G_GAN: 0.8742, G_L1: 23.6952\nEpoch [5] completed - Avg G Loss: 19.7062, Avg D Loss: 0.1543\nEpoch [6/100], Step [0], D Loss: 0.6416, G Loss: 20.8834, G_GAN: 0.5077, G_L1: 20.3757\nEpoch [7] completed - Avg G Loss: 17.6040, Avg D Loss: 0.1668\nEpoch [8/100], Step [0], D Loss: 0.2450, G Loss: 18.9419, G_GAN: 0.8038, G_L1: 18.1381\nEpoch [8] completed - Avg G Loss: 16.9322, Avg D Loss: 0.2080\nEpoch [9/100], Step [0], D Loss: 0.1088, G Loss: 17.1494, G_GAN: 0.5520, G_L1: 16.5974\nEpoch [9] completed - Avg G Loss: 16.8770, Avg D Loss: 0.2068\nEpoch [10/100], Step [0], D Loss: 0.1609, G Loss: 16.7525, G_GAN: 0.8317, G_L1: 15.9208\nEpoch [10] completed - Avg G Loss: 16.0102, Avg D Loss: 0.2890\nEpoch [11/100], Step [0], D Loss: 0.2441, G Loss: 11.5252, G_GAN: 0.3583, G_L1: 11.1669\nEpoch [11] completed - Avg G Loss: 15.2067, Avg D Loss: 0.2246\nEpoch [12/100], Step [0], D Loss: 0.3645, G Loss: 10.6763, G_GAN: 0.5545, G_L1: 10.1218\nEpoch [12] completed - Avg G Loss: 14.8369, Avg D Loss: 0.2262\nEpoch [13/100], Step [0], D Loss: 0.1873, G Loss: 14.9613, G_GAN: 0.5265, G_L1: 14.4348\nEpoch [13] completed - Avg G Loss: 14.4269, Avg D Loss: 0.2841\nEpoch [14/100], Step [0], D Loss: 0.1917, G Loss: 8.2445, G_GAN: 0.2832, G_L1: 7.9613\nEpoch [14] completed - Avg G Loss: 14.2334, Avg D Loss: 0.2297\nEpoch [15/100], Step [0], D Loss: 0.2884, G Loss: 10.9397, G_GAN: 0.0575, G_L1: 10.8821\nEpoch [15] completed - Avg G Loss: 13.8337, Avg D Loss: 0.3116\nEpoch [16/100], Step [0], D Loss: 0.2464, G Loss: 11.7302, G_GAN: 0.3494, G_L1: 11.3808\nEpoch [16] completed - Avg G Loss: 13.7611, Avg D Loss: 0.2234\nEpoch [17/100], Step [0], D Loss: 0.2917, G Loss: 12.6510, G_GAN: 0.7322, G_L1: 11.9189\nEpoch [17] completed - Avg G Loss: 13.4565, Avg D Loss: 0.1930\nEpoch [18/100], Step [0], D Loss: 0.0764, G Loss: 11.7854, G_GAN: 0.2996, G_L1: 11.4858\nEpoch [18] completed - Avg G Loss: 13.6261, Avg D Loss: 0.2293\nEpoch [19/100], Step [0], D Loss: 0.1925, G Loss: 11.4283, G_GAN: 0.5911, G_L1: 10.8373\nEpoch [19] completed - Avg G Loss: 13.0610, Avg D Loss: 0.2099\nEpoch [20/100], Step [0], D Loss: 0.2056, G Loss: 12.5006, G_GAN: 0.3310, G_L1: 12.1696\nEpoch [20] completed - Avg G Loss: 12.6926, Avg D Loss: 0.2274\nEpoch [21/100], Step [0], D Loss: 0.1357, G Loss: 19.7767, G_GAN: 0.5166, G_L1: 19.2601\nEpoch [21] completed - Avg G Loss: 12.9515, Avg D Loss: 0.4243\nEpoch [22/100], Step [0], D Loss: 0.3255, G Loss: 9.2928, G_GAN: 0.3617, G_L1: 8.9311\nEpoch [22] completed - Avg G Loss: 12.5036, Avg D Loss: 0.2855\nEpoch [23/100], Step [0], D Loss: 0.2482, G Loss: 11.7336, G_GAN: 0.2464, G_L1: 11.4872\nEpoch [23] completed - Avg G Loss: 12.3899, Avg D Loss: 0.2557\nEpoch [24/100], Step [0], D Loss: 0.2413, G Loss: 18.8104, G_GAN: 0.3141, G_L1: 18.4963\nEpoch [24] completed - Avg G Loss: 11.8509, Avg D Loss: 0.2537\nEpoch [25/100], Step [0], D Loss: 0.2691, G Loss: 10.7436, G_GAN: 0.3318, G_L1: 10.4118\nEpoch [25] completed - Avg G Loss: 12.0282, Avg D Loss: 0.2384\nEpoch [26/100], Step [0], D Loss: 0.2177, G Loss: 8.1003, G_GAN: 0.1674, G_L1: 7.9329\nEpoch [26] completed - Avg G Loss: 11.7101, Avg D Loss: 0.2463\nEpoch [27/100], Step [0], D Loss: 0.2789, G Loss: 11.5458, G_GAN: 0.5484, G_L1: 10.9974\nEpoch [27] completed - Avg G Loss: 11.5798, Avg D Loss: 0.2383\nEpoch [28/100], Step [0], D Loss: 0.2771, G Loss: 6.7997, G_GAN: 0.3877, G_L1: 6.4120\nEpoch [28] completed - Avg G Loss: 11.8583, Avg D Loss: 0.2289\nEpoch [29/100], Step [0], D Loss: 0.2906, G Loss: 12.5361, G_GAN: 0.7310, G_L1: 11.8051\nEpoch [29] completed - Avg G Loss: 11.6992, Avg D Loss: 0.2322\nEpoch [30/100], Step [0], D Loss: 0.1846, G Loss: 15.2162, G_GAN: 0.3229, G_L1: 14.8933\nEpoch [30] completed - Avg G Loss: 11.7364, Avg D Loss: 0.2275\nEpoch [31/100], Step [0], D Loss: 0.2285, G Loss: 13.5292, G_GAN: 0.4658, G_L1: 13.0634\nEpoch [31] completed - Avg G Loss: 11.5305, Avg D Loss: 0.2432\nEpoch [32/100], Step [0], D Loss: 0.1785, G Loss: 12.6897, G_GAN: 0.4241, G_L1: 12.2655\nEpoch [32] completed - Avg G Loss: 11.6231, Avg D Loss: 0.1999\nEpoch [33/100], Step [0], D Loss: 0.4194, G Loss: 10.9753, G_GAN: 1.0795, G_L1: 9.8957\nEpoch [33] completed - Avg G Loss: 11.6735, Avg D Loss: 0.2298\nEpoch [34/100], Step [0], D Loss: 0.2937, G Loss: 12.0158, G_GAN: 0.5041, G_L1: 11.5117\nEpoch [34] completed - Avg G Loss: 11.3976, Avg D Loss: 0.2174\nEpoch [35/100], Step [0], D Loss: 0.2092, G Loss: 16.5586, G_GAN: 0.6459, G_L1: 15.9127\nEpoch [35] completed - Avg G Loss: 11.4634, Avg D Loss: 0.2205\nEpoch [36/100], Step [0], D Loss: 0.2187, G Loss: 13.9054, G_GAN: 0.6282, G_L1: 13.2772\nEpoch [36] completed - Avg G Loss: 11.4373, Avg D Loss: 0.2267\nEpoch [37/100], Step [0], D Loss: 0.1372, G Loss: 10.3334, G_GAN: 0.6302, G_L1: 9.7032\nEpoch [37] completed - Avg G Loss: 11.4546, Avg D Loss: 0.2005\nEpoch [38/100], Step [0], D Loss: 0.1680, G Loss: 8.4629, G_GAN: 0.4281, G_L1: 8.0348\nEpoch [38] completed - Avg G Loss: 11.4015, Avg D Loss: 0.1797\nEpoch [39/100], Step [0], D Loss: 0.2696, G Loss: 14.5710, G_GAN: 0.2720, G_L1: 14.2991\nEpoch [39] completed - Avg G Loss: 11.0508, Avg D Loss: 0.1957\nEpoch [40/100], Step [0], D Loss: 0.0938, G Loss: 13.9912, G_GAN: 0.4878, G_L1: 13.5035\nEpoch [40] completed - Avg G Loss: 10.6786, Avg D Loss: 0.2326\nEpoch [41/100], Step [0], D Loss: 0.2299, G Loss: 13.0022, G_GAN: 0.4123, G_L1: 12.5899\nEpoch [41] completed - Avg G Loss: 10.9548, Avg D Loss: 0.1774\nEpoch [42/100], Step [0], D Loss: 0.2841, G Loss: 8.9893, G_GAN: 0.4494, G_L1: 8.5399\nEpoch [42] completed - Avg G Loss: 10.9033, Avg D Loss: 0.1753\nEpoch [43/100], Step [0], D Loss: 0.1698, G Loss: 9.4811, G_GAN: 0.5743, G_L1: 8.9068\nEpoch [43] completed - Avg G Loss: 10.7043, Avg D Loss: 0.1851\nEpoch [44/100], Step [0], D Loss: 0.2316, G Loss: 11.5338, G_GAN: 0.4356, G_L1: 11.0983\nEpoch [44] completed - Avg G Loss: 11.0483, Avg D Loss: 0.2036\nEpoch [45/100], Step [0], D Loss: 0.1491, G Loss: 8.9491, G_GAN: 1.5835, G_L1: 7.3656\nEpoch [45] completed - Avg G Loss: 10.9379, Avg D Loss: 0.1970\nEpoch [46/100], Step [0], D Loss: 0.0815, G Loss: 8.8871, G_GAN: 0.8803, G_L1: 8.0068\nEpoch [46] completed - Avg G Loss: 10.8524, Avg D Loss: 0.1594\nEpoch [47/100], Step [0], D Loss: 0.2751, G Loss: 7.8769, G_GAN: 0.1060, G_L1: 7.7709\nEpoch [47] completed - Avg G Loss: 10.7049, Avg D Loss: 0.1914\nEpoch [48/100], Step [0], D Loss: 0.0647, G Loss: 10.0413, G_GAN: 0.8451, G_L1: 9.1961\nEpoch [48] completed - Avg G Loss: 10.7230, Avg D Loss: 0.2061\nEpoch [49/100], Step [0], D Loss: 0.4363, G Loss: 7.4117, G_GAN: 0.7022, G_L1: 6.7095\nEpoch [49] completed - Avg G Loss: 10.5988, Avg D Loss: 0.1775\nEpoch [50/100], Step [0], D Loss: 0.0823, G Loss: 10.5048, G_GAN: 0.9267, G_L1: 9.5781\nEpoch [50] completed - Avg G Loss: 10.4641, Avg D Loss: 0.1459\nEpoch [51/100], Step [0], D Loss: 0.0274, G Loss: 8.8823, G_GAN: 0.8432, G_L1: 8.0391\nEpoch [51] completed - Avg G Loss: 10.5211, Avg D Loss: 0.1349\nEpoch [52/100], Step [0], D Loss: 0.1645, G Loss: 9.6217, G_GAN: 0.3443, G_L1: 9.2774\nEpoch [52] completed - Avg G Loss: 10.5204, Avg D Loss: 0.0847\nEpoch [53/100], Step [0], D Loss: 0.0682, G Loss: 13.3131, G_GAN: 0.8813, G_L1: 12.4318\nEpoch [53] completed - Avg G Loss: 10.4355, Avg D Loss: 0.1535\nEpoch [54/100], Step [0], D Loss: 0.1298, G Loss: 11.6649, G_GAN: 0.5768, G_L1: 11.0881\nEpoch [54] completed - Avg G Loss: 10.3559, Avg D Loss: 0.1596\nEpoch [55/100], Step [0], D Loss: 0.0758, G Loss: 10.5140, G_GAN: 0.6498, G_L1: 9.8643\nEpoch [55] completed - Avg G Loss: 10.3973, Avg D Loss: 0.1016\nEpoch [56/100], Step [0], D Loss: 0.1163, G Loss: 8.4064, G_GAN: 0.2356, G_L1: 8.1708\nEpoch [56] completed - Avg G Loss: 10.2552, Avg D Loss: 0.1485\nEpoch [57/100], Step [0], D Loss: 0.3088, G Loss: 7.1762, G_GAN: 0.2257, G_L1: 6.9505\nEpoch [57] completed - Avg G Loss: 10.3764, Avg D Loss: 0.1270\nEpoch [58/100], Step [0], D Loss: 0.1134, G Loss: 8.8771, G_GAN: 0.4372, G_L1: 8.4398\nEpoch [58] completed - Avg G Loss: 10.1239, Avg D Loss: 0.1700\nEpoch [59/100], Step [0], D Loss: 0.0538, G Loss: 10.7127, G_GAN: 0.6356, G_L1: 10.0771\nEpoch [59] completed - Avg G Loss: 10.1451, Avg D Loss: 0.1629\nEpoch [60/100], Step [0], D Loss: 0.1948, G Loss: 5.6986, G_GAN: 0.2815, G_L1: 5.4171\nEpoch [60] completed - Avg G Loss: 10.0913, Avg D Loss: 0.1422\nEpoch [61/100], Step [0], D Loss: 0.1175, G Loss: 9.0106, G_GAN: 0.7363, G_L1: 8.2743\nEpoch [61] completed - Avg G Loss: 10.1779, Avg D Loss: 0.1584\nEpoch [62/100], Step [0], D Loss: 0.0525, G Loss: 8.5722, G_GAN: 1.4383, G_L1: 7.1339\nEpoch [62] completed - Avg G Loss: 10.1065, Avg D Loss: 0.1355\nEpoch [63/100], Step [0], D Loss: 0.2152, G Loss: 10.2080, G_GAN: 0.5197, G_L1: 9.6883\nEpoch [63] completed - Avg G Loss: 10.0431, Avg D Loss: 0.2015\nEpoch [64/100], Step [0], D Loss: 0.1268, G Loss: 7.7133, G_GAN: 0.4905, G_L1: 7.2228\nEpoch [64] completed - Avg G Loss: 10.2255, Avg D Loss: 0.1214\nEpoch [65/100], Step [0], D Loss: 0.0865, G Loss: 9.4838, G_GAN: 0.6188, G_L1: 8.8650\nEpoch [65] completed - Avg G Loss: 10.3676, Avg D Loss: 0.1292\nEpoch [66/100], Step [0], D Loss: 0.0143, G Loss: 9.3854, G_GAN: 1.1109, G_L1: 8.2745\nEpoch [66] completed - Avg G Loss: 10.2596, Avg D Loss: 0.1806\nEpoch [67/100], Step [0], D Loss: 0.0835, G Loss: 9.7355, G_GAN: 0.8765, G_L1: 8.8590\nEpoch [67] completed - Avg G Loss: 9.9000, Avg D Loss: 0.1381\nEpoch [68/100], Step [0], D Loss: 0.1304, G Loss: 9.9042, G_GAN: 0.3088, G_L1: 9.5954\nEpoch [68] completed - Avg G Loss: 9.9635, Avg D Loss: 0.1318\nEpoch [69/100], Step [0], D Loss: 0.1148, G Loss: 7.3490, G_GAN: 0.1000, G_L1: 7.2489\nEpoch [69] completed - Avg G Loss: 9.6171, Avg D Loss: 0.1564\nEpoch [70/100], Step [0], D Loss: 0.0681, G Loss: 9.4336, G_GAN: 0.6013, G_L1: 8.8323\nEpoch [70] completed - Avg G Loss: 9.9499, Avg D Loss: 0.1344\nEpoch [71/100], Step [0], D Loss: 0.0304, G Loss: 8.7032, G_GAN: 0.8187, G_L1: 7.8845\nEpoch [71] completed - Avg G Loss: 10.0026, Avg D Loss: 0.1245\nEpoch [72/100], Step [0], D Loss: 0.2253, G Loss: 8.3242, G_GAN: 0.3566, G_L1: 7.9677\nEpoch [72] completed - Avg G Loss: 9.8885, Avg D Loss: 0.1441\nEpoch [73/100], Step [0], D Loss: 0.0960, G Loss: 11.0646, G_GAN: 0.8379, G_L1: 10.2267\nEpoch [73] completed - Avg G Loss: 9.9321, Avg D Loss: 0.1490\nEpoch [74/100], Step [0], D Loss: 0.1127, G Loss: 8.2574, G_GAN: 0.4699, G_L1: 7.7874\nEpoch [74] completed - Avg G Loss: 9.8354, Avg D Loss: 0.1194\nEpoch [75/100], Step [0], D Loss: 0.1339, G Loss: 10.3807, G_GAN: 0.3695, G_L1: 10.0112\nEpoch [75] completed - Avg G Loss: 10.0324, Avg D Loss: 0.1310\nEpoch [76/100], Step [0], D Loss: 0.9209, G Loss: 10.4294, G_GAN: 0.0144, G_L1: 10.4150\nEpoch [76] completed - Avg G Loss: 9.8121, Avg D Loss: 0.1737\nEpoch [77/100], Step [0], D Loss: 0.0365, G Loss: 10.9649, G_GAN: 0.8719, G_L1: 10.0931\nEpoch [77] completed - Avg G Loss: 9.9550, Avg D Loss: 0.1328\nEpoch [78/100], Step [0], D Loss: 0.1570, G Loss: 8.7757, G_GAN: 0.3634, G_L1: 8.4122\nEpoch [78] completed - Avg G Loss: 9.5626, Avg D Loss: 0.1410\nEpoch [79/100], Step [0], D Loss: 0.2218, G Loss: 5.8802, G_GAN: 0.0522, G_L1: 5.8280\nEpoch [79] completed - Avg G Loss: 9.4871, Avg D Loss: 0.1329\nEpoch [80/100], Step [0], D Loss: 0.1351, G Loss: 8.8261, G_GAN: 0.7435, G_L1: 8.0826\nEpoch [80] completed - Avg G Loss: 9.8363, Avg D Loss: 0.1538\nEpoch [81/100], Step [0], D Loss: 0.1845, G Loss: 8.6907, G_GAN: 0.4866, G_L1: 8.2042\nEpoch [81] completed - Avg G Loss: 9.5119, Avg D Loss: 0.1290\nEpoch [82/100], Step [0], D Loss: 0.2627, G Loss: 7.2553, G_GAN: 0.4661, G_L1: 6.7892\nEpoch [82] completed - Avg G Loss: 9.5296, Avg D Loss: 0.1192\nEpoch [83/100], Step [0], D Loss: 0.1145, G Loss: 12.9429, G_GAN: 1.1533, G_L1: 11.7896\nEpoch [83] completed - Avg G Loss: 9.6539, Avg D Loss: 0.1170\nEpoch [84/100], Step [0], D Loss: 0.0416, G Loss: 7.5738, G_GAN: 0.5965, G_L1: 6.9774\nEpoch [84] completed - Avg G Loss: 9.6143, Avg D Loss: 0.1138\nEpoch [85/100], Step [0], D Loss: 0.0980, G Loss: 10.3891, G_GAN: 0.5032, G_L1: 9.8859\nEpoch [85] completed - Avg G Loss: 9.4998, Avg D Loss: 0.1669\nEpoch [86/100], Step [0], D Loss: 0.1578, G Loss: 10.5664, G_GAN: 0.8896, G_L1: 9.6768\nEpoch [86] completed - Avg G Loss: 9.4729, Avg D Loss: 0.1316\nEpoch [87/100], Step [0], D Loss: 0.1823, G Loss: 7.6535, G_GAN: 0.2331, G_L1: 7.4204\nEpoch [87] completed - Avg G Loss: 9.5428, Avg D Loss: 0.1229\nEpoch [88/100], Step [0], D Loss: 0.0553, G Loss: 12.2520, G_GAN: 0.6811, G_L1: 11.5709\nEpoch [88] completed - Avg G Loss: 9.5635, Avg D Loss: 0.1474\nEpoch [89/100], Step [0], D Loss: 0.1201, G Loss: 7.7836, G_GAN: 0.6434, G_L1: 7.1403\nEpoch [89] completed - Avg G Loss: 9.4725, Avg D Loss: 0.1245\nEpoch [90/100], Step [0], D Loss: 0.1631, G Loss: 11.3281, G_GAN: 0.3959, G_L1: 10.9322\nEpoch [90] completed - Avg G Loss: 9.5303, Avg D Loss: 0.1119\nEpoch [91/100], Step [0], D Loss: 0.0924, G Loss: 9.3224, G_GAN: 0.6856, G_L1: 8.6368\nEpoch [91] completed - Avg G Loss: 9.6326, Avg D Loss: 0.1041\nEpoch [92/100], Step [0], D Loss: 0.0108, G Loss: 11.8622, G_GAN: 0.5642, G_L1: 11.2981\nEpoch [92] completed - Avg G Loss: 9.3154, Avg D Loss: 0.1290\nEpoch [93/100], Step [0], D Loss: 0.0710, G Loss: 7.5086, G_GAN: 0.7815, G_L1: 6.7271\nEpoch [93] completed - Avg G Loss: 9.2753, Avg D Loss: 0.1417\nEpoch [94/100], Step [0], D Loss: 0.4532, G Loss: 6.2052, G_GAN: 0.3494, G_L1: 5.8558\nEpoch [94] completed - Avg G Loss: 9.2960, Avg D Loss: 0.1140\nEpoch [95/100], Step [0], D Loss: 0.0088, G Loss: 10.4300, G_GAN: 0.9908, G_L1: 9.4392\nEpoch [95] completed - Avg G Loss: 9.5416, Avg D Loss: 0.1006\nEpoch [96/100], Step [0], D Loss: 0.1496, G Loss: 10.1644, G_GAN: 1.4744, G_L1: 8.6900\nEpoch [96] completed - Avg G Loss: 9.2938, Avg D Loss: 0.1533\nEpoch [97/100], Step [0], D Loss: 0.2468, G Loss: 6.7772, G_GAN: 0.5792, G_L1: 6.1979\nEpoch [97] completed - Avg G Loss: 8.9584, Avg D Loss: 0.1365\nEpoch [98/100], Step [0], D Loss: 0.1147, G Loss: 7.7990, G_GAN: 0.7463, G_L1: 7.0527\nEpoch [98] completed - Avg G Loss: 9.1808, Avg D Loss: 0.1254\nEpoch [99/100], Step [0], D Loss: 0.0231, G Loss: 12.8360, G_GAN: 0.9240, G_L1: 11.9120\nEpoch [99] completed - Avg G Loss: 9.1914, Avg D Loss: 0.1228\n","output_type":"stream"}],"execution_count":4}]}